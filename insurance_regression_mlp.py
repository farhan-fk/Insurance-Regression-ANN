# -*- coding: utf-8 -*-
"""Insurance Regression MLP.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/158FQcmWPNTOGnoZ0Gk5KXy9VXr7hycjS
"""

import pandas as pd

# Assuming the uploaded file is named 'your_file.csv'
df = pd.read_csv('insurance.csv')

print(df.head())

features=df.iloc[:,0:6]

print(features.head())

labels=df.iloc[:,-1]

print(labels.head())

print(("Number of features: ", features.shape))

print(labels.describe())

features  = pd.get_dummies(features)

from sklearn.model_selection import train_test_split
features_train, features_test, labels_train, labels_test = train_test_split(features, labels, test_size=0.33, random_state=42)

from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import StandardScaler
my_ct= ColumnTransformer([('scale', StandardScaler(), ['age', 'bmi', 'children'])], remainder='passthrough')
features_train_scale=my_ct.fit_transform(features_train)
features_test_scale=my_ct.transform(features_test)
features_train_scale = pd.DataFrame(features_train_scale, columns = features_train.columns)
features_test_scale= pd.DataFrame(features_test_scale, columns = features_train.columns)

print(features_train_scale.shape[1])

from tensorflow.keras.models import Sequential

from tensorflow.keras import layers
import tensorflow as tf
from tensorflow.keras.layers import InputLayer
from tensorflow.keras.layers import Dense
from tensorflow.keras.optimizers import Adam
def design_model(features):
  model = Sequential(name = "my_first_model")
  input = InputLayer(input_shape=(features.shape[1],))
  #add the input layer
  model.add(input)
  #your code here
  model.add(Dense(128, activation = 'relu'))
  #adding an output layer to our model
  model.add(Dense(1))
  opt = Adam(learning_rate = 0.01)
  model.compile(loss='mse',  metrics=['mae'], optimizer=opt)
  return model

#invoke the function for our model design
model = design_model(features_train_scale)

#print the model summary here
print(model.summary())

model.fit(features_train_scale, labels_train, epochs = 40, batch_size = 1, verbose = 1)

#evaluate the model on the test data
val_mse, val_mae = model.evaluate(features_test_scale, labels_test, verbose = 0)
print("MAE: ", val_mae)

# Assuming 'new_data' is your new dataset (similar structure to the training data)
new_data = pd.DataFrame({
    'age': [19, 56, 40],
    'sex': ['male', 'female', 'female'],
    'bmi': [24.6, 40.3, 30.1],
    'children': [1, 0, 2],
    'smoker': ['no', 'no', 'yes'],
    'region': ['southwest', 'southwest', 'southeast']
})

# Apply one-hot encoding for 'region'
new_data_encoded = pd.get_dummies(new_data, columns=['region'], drop_first=True)

# Ensure that all the necessary columns are present, add them if not
missing_columns = set(features_train.columns) - set(new_data_encoded.columns)
for col in missing_columns:
    new_data_encoded[col] = 0

# Reorder columns to match the order during training
new_data_encoded = new_data_encoded[features_train.columns]

# One-hot encode categorical variables and standardize numeric features
new_data_preprocessed = my_ct.transform(new_data_encoded)

# Make predictions
new_data_predictions = model.predict(new_data_preprocessed)

# Display the predictions
print("New Data Predictions:")
print(new_data_predictions.flatten())

